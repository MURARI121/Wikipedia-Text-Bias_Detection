{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, TweetTokenizer \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from afinn import Afinn\n",
    "af = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed =pd.read_csv('../Data/feature.csv')\n",
    "truth =pd.read_csv('../Data/ground_truth.csv')\n",
    "truth = np.array(truth.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "columns=['POS-2','POS-1','POS','POS+1','POS+2']\n",
    "for index, row in preprocessed.iterrows():\n",
    "    text=row['POS'].split()\n",
    "    for t in range(len(text)):\n",
    "        value=[]\n",
    "        for i in range(t-2,t+3):\n",
    "            if((i<0)|(i>=len(text))):\n",
    "                value.append(\"None\")\n",
    "            else:\n",
    "                value.append(text[i])\n",
    "        zipped = zip(columns, value)\n",
    "        a_dictionary = dict(zipped)\n",
    "        data.append(a_dictionary)\n",
    "POS = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "columns=['id','text', 'positive', 'strong_subjectives', 'implicatives', 'npov_word',\n",
    "         'factives', 'assertives', 'weak_subjectives', 'hedges', 'entailment',\n",
    "         'negative', 'report_verbs', 'lemma', 'position', 'grammatical','polarity']\n",
    "for index, row in preprocessed.iterrows():\n",
    "    text=row['text'].split()\n",
    "    for t in range(len(text)):\n",
    "        value=[]\n",
    "        value.append(index)\n",
    "        for column in columns[1:]:\n",
    "            value.append(row[column].split()[t])\n",
    "        zipped = zip(columns, value)\n",
    "        a_dictionary = dict(zipped)\n",
    "        data.append(a_dictionary)\n",
    "\n",
    "word_values = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "columns=['positive', 'strong_subjectives', 'implicatives','factives', 'assertives',\n",
    "         'weak_subjectives', 'hedges', 'entailment','negative', 'report_verbs']\n",
    "col=[i+\"_around\" for i in columns]\n",
    "for index, row in preprocessed.iterrows():\n",
    "    text=row['text'].split()\n",
    "    for t in range(len(text)):\n",
    "        value=[]\n",
    "        for column in columns:\n",
    "            p=[row[column].split()[i] for i in range(t-2,t+3) if (i>=0)&(i<len(text))&(i!=t)]\n",
    "            value.append(\"True\" if \"True\" in p else \"False\")        \n",
    "        zipped = zip(col, value)\n",
    "        a_dictionary = dict(zipped)\n",
    "        data.append(a_dictionary)\n",
    "\n",
    "word_surrounding = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for index, row in preprocessed.iterrows():\n",
    "    text=row['text'].split()\n",
    "    for t in range(len(text)):\n",
    "        value = [1 if (truth[index]==text[t]) else 0]        \n",
    "        data.append(value)\n",
    "\n",
    "label = pd.DataFrame.from_dict(data)\n",
    "label.columns=['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.concat([word_values,POS, word_surrounding,label], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=trainset['text']\n",
    "lemma=trainset['lemma']\n",
    "train= trainset.drop(['text','lemma'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "path = \"../Data/GoogleNews-vectors-negative300.bin\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec=[]\n",
    "col=[\"word_vec_\"+str(i) for i in range(300)]\n",
    "\n",
    "for i in range(len(word)):\n",
    "    \n",
    "    if word[i] in model.vocab:\n",
    "        w = model[word[i]]\n",
    "    else:\n",
    "        w=np.zeros(300)\n",
    "    \n",
    "    zipped = zip(col, w)\n",
    "    a_dictionary = dict(zipped)\n",
    "    word_vec.append(a_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemma=[]\n",
    "col=[\"lemma_vec_\"+str(i) for i in range(300)]\n",
    "\n",
    "for i in range(len(lemma)):\n",
    "    if lemma[i] in model.vocab:\n",
    "        w = model[lemma[i]]\n",
    "    else:\n",
    "        w=np.zeros(300)\n",
    "\n",
    "    zipped = zip(col, w)\n",
    "    a_dictionary = dict(zipped)\n",
    "    word_lemma.append(a_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec=pd.DataFrame(word_vec)\n",
    "word_lemma=pd.DataFrame(word_lemma)\n",
    "word=pd.DataFrame(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_encoded = train.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded =pd.concat([word,word_vec,word_lemma,train_encoded],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1340980"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_encoded.to_pickle('../Data/train_encoded.pickle')\n",
    "# df2 = pd.read_pickle('my_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle] *",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
